{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Dropout , Linear \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 'Jigsaw Toxicity/train.csv'\n",
    "TEST = 'Jigsaw Toxicity/test.csv'\n",
    "TEST_LABEL = 'Jigsaw Toxicity/test_labels.csv'\n",
    "SAMPLE = 'Jigsaw Toxicity/sample_submission.csv'\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(TRAIN)\n",
    "test_csv = pd.read_csv(TEST)\n",
    "test_label = pd.read_csv(TEST_LABEL)\n",
    "sample_sub = pd.read_csv(SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_preprocessing(text):\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text) #remove mentions or usernames in the text.\n",
    "    text = re.sub(r'[0-9]+' , '' ,text) # removes any sequence of digits\n",
    "\n",
    "    text = re.sub(r'\\s([@][\\w_-]+)', '', text).strip() \n",
    "    text = re.sub(r'&amp;', '&', text) #replaces the HTML entity '&' with an ampersand '&'\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() #extra whitespace\n",
    "    text = text.replace(\"#\" , \" \")\n",
    "    encoded_string = text.encode(\"ascii\", \"ignore\") #encodes the text into ASCII format\n",
    "    decode_string = encoded_string.decode() #decodes the ASCII-encoded text back into a string.\n",
    "    return decode_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :  == From RfC == \n",
      "\n",
      " The title is fine as it is, IMO.\n",
      "Processed : == From RfC == The title is fine as it is, IMO.\n"
     ]
    }
   ],
   "source": [
    "print('Original : ' ,test_csv['comment_text'][1])\n",
    "print('Processed :' , text_preprocessing(test_csv['comment_text'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toxic = train_csv[train_csv['toxic'] == 1]['comment_text'].values\n",
    "X_non_toxic = train_csv[train_csv['toxic'] == 0]['comment_text'].sample(15294).values\n",
    "\n",
    "Y_toxic = train_csv[train_csv['toxic'] == 1]['toxic'].values\n",
    "Y_non_toxic = train_csv[train_csv['toxic'] == 0]['toxic'].sample(15294).values\n",
    "\n",
    "X = np.concatenate([X_toxic, X_non_toxic])\n",
    "Y = np.concatenate([Y_toxic, Y_non_toxic])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57638ee67ecf48fbbf4d9fc5fd5f08f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa41ef9cb6745c5b32b8fb10337f0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abaa885e7d8a43c48b7a535c3f636133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7441b238a7401baea824c0997fa08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            truncation = True,\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
      "Token IDs:  [101, 10338, 6342, 9102, 2077, 2017, 18138, 2105, 2006, 2026, 2147, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 300\n",
    "\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class DistilBertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(DistilBertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        # self.LSTM = nn.LSTM(D_in,D_in,bidirectional=True)\n",
    "        # self.clf = nn.Linear(D_in*2,2)\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.LSTM(D_in,D_in)\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    distilbert_classifier = DistilBertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    distilbert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(distilbert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return distilbert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04168205338b4b71b36cc13857781750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.427060   |     -      |     -     |   6.67   \n",
      "   1    |   40    |   0.248961   |     -      |     -     |   4.15   \n",
      "   1    |   60    |   0.229559   |     -      |     -     |   4.15   \n",
      "   1    |   80    |   0.169606   |     -      |     -     |   4.34   \n",
      "   1    |   100   |   0.187114   |     -      |     -     |   4.38   \n",
      "   1    |   120   |   0.246449   |     -      |     -     |   4.29   \n",
      "   1    |   140   |   0.208580   |     -      |     -     |   4.16   \n",
      "   1    |   160   |   0.201878   |     -      |     -     |   4.16   \n",
      "   1    |   180   |   0.233902   |     -      |     -     |   4.17   \n",
      "   1    |   200   |   0.159040   |     -      |     -     |   4.18   \n",
      "   1    |   220   |   0.162073   |     -      |     -     |   4.18   \n",
      "   1    |   240   |   0.186623   |     -      |     -     |   4.18   \n",
      "   1    |   260   |   0.185359   |     -      |     -     |   4.21   \n",
      "   1    |   280   |   0.190630   |     -      |     -     |   4.24   \n",
      "   1    |   300   |   0.182702   |     -      |     -     |   4.21   \n",
      "   1    |   320   |   0.178564   |     -      |     -     |   4.26   \n",
      "   1    |   340   |   0.206109   |     -      |     -     |   4.23   \n",
      "   1    |   360   |   0.126255   |     -      |     -     |   4.25   \n",
      "   1    |   380   |   0.218264   |     -      |     -     |   4.22   \n",
      "   1    |   400   |   0.218999   |     -      |     -     |   4.23   \n",
      "   1    |   420   |   0.164750   |     -      |     -     |   4.24   \n",
      "   1    |   440   |   0.159393   |     -      |     -     |   4.24   \n",
      "   1    |   460   |   0.159081   |     -      |     -     |   4.22   \n",
      "   1    |   480   |   0.172807   |     -      |     -     |   4.22   \n",
      "   1    |   500   |   0.184455   |     -      |     -     |   4.22   \n",
      "   1    |   520   |   0.203048   |     -      |     -     |   4.25   \n",
      "   1    |   540   |   0.220585   |     -      |     -     |   4.24   \n",
      "   1    |   560   |   0.138182   |     -      |     -     |   4.24   \n",
      "   1    |   580   |   0.154853   |     -      |     -     |   4.24   \n",
      "   1    |   600   |   0.187781   |     -      |     -     |   4.22   \n",
      "   1    |   620   |   0.171979   |     -      |     -     |   4.21   \n",
      "   1    |   640   |   0.182248   |     -      |     -     |   4.21   \n",
      "   1    |   660   |   0.153583   |     -      |     -     |   4.23   \n",
      "   1    |   680   |   0.197741   |     -      |     -     |   4.23   \n",
      "   1    |   700   |   0.125062   |     -      |     -     |   4.23   \n",
      "   1    |   720   |   0.174376   |     -      |     -     |   4.23   \n",
      "   1    |   740   |   0.130631   |     -      |     -     |   4.25   \n",
      "   1    |   760   |   0.147326   |     -      |     -     |   4.27   \n",
      "   1    |   764   |   0.318035   |     -      |     -     |   0.80   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.190342   |  0.159270  |   93.71   |  177.86  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.105767   |     -      |     -     |   4.45   \n",
      "   2    |   40    |   0.140957   |     -      |     -     |   4.24   \n",
      "   2    |   60    |   0.102947   |     -      |     -     |   4.23   \n",
      "   2    |   80    |   0.110648   |     -      |     -     |   4.22   \n",
      "   2    |   100   |   0.076748   |     -      |     -     |   4.24   \n",
      "   2    |   120   |   0.067081   |     -      |     -     |   4.23   \n",
      "   2    |   140   |   0.126308   |     -      |     -     |   4.25   \n",
      "   2    |   160   |   0.071299   |     -      |     -     |   4.24   \n",
      "   2    |   180   |   0.068361   |     -      |     -     |   4.25   \n",
      "   2    |   200   |   0.072082   |     -      |     -     |   4.24   \n",
      "   2    |   220   |   0.148343   |     -      |     -     |   4.25   \n",
      "   2    |   240   |   0.081075   |     -      |     -     |   4.23   \n",
      "   2    |   260   |   0.111904   |     -      |     -     |   4.24   \n",
      "   2    |   280   |   0.087224   |     -      |     -     |   4.24   \n",
      "   2    |   300   |   0.129863   |     -      |     -     |   4.23   \n",
      "   2    |   320   |   0.084641   |     -      |     -     |   4.24   \n",
      "   2    |   340   |   0.087968   |     -      |     -     |   4.24   \n",
      "   2    |   360   |   0.112210   |     -      |     -     |   4.24   \n",
      "   2    |   380   |   0.078999   |     -      |     -     |   4.25   \n",
      "   2    |   400   |   0.093450   |     -      |     -     |   4.23   \n",
      "   2    |   420   |   0.084103   |     -      |     -     |   4.24   \n",
      "   2    |   440   |   0.078164   |     -      |     -     |   4.23   \n",
      "   2    |   460   |   0.080762   |     -      |     -     |   4.24   \n",
      "   2    |   480   |   0.076824   |     -      |     -     |   4.23   \n",
      "   2    |   500   |   0.085948   |     -      |     -     |   4.23   \n",
      "   2    |   520   |   0.090359   |     -      |     -     |   4.23   \n",
      "   2    |   540   |   0.087811   |     -      |     -     |   4.24   \n",
      "   2    |   560   |   0.077229   |     -      |     -     |   4.23   \n",
      "   2    |   580   |   0.070404   |     -      |     -     |   4.22   \n",
      "   2    |   600   |   0.082072   |     -      |     -     |   4.24   \n",
      "   2    |   620   |   0.069572   |     -      |     -     |   4.25   \n",
      "   2    |   640   |   0.114563   |     -      |     -     |   4.24   \n",
      "   2    |   660   |   0.104906   |     -      |     -     |   4.22   \n",
      "   2    |   680   |   0.078351   |     -      |     -     |   4.22   \n",
      "   2    |   700   |   0.061561   |     -      |     -     |   4.24   \n",
      "   2    |   720   |   0.125454   |     -      |     -     |   4.25   \n",
      "   2    |   740   |   0.101154   |     -      |     -     |   4.24   \n",
      "   2    |   760   |   0.061739   |     -      |     -     |   4.24   \n",
      "   2    |   764   |   0.026554   |     -      |     -     |   0.78   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.091489   |  0.167217  |   94.72   |  175.93  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "distilbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(distilbert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distilbert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC, accuracy, precision, recall, and F-measure on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Get precision, recall, and F-measure\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f_measure = f1_score(y_true, y_pred)\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F-measure: {f_measure:.4f}')\n",
    "\n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9874\n",
      "Accuracy: 94.77%\n",
      "Precision: 0.9372\n",
      "Recall: 0.9583\n",
      "F-measure: 0.9476\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = distilbert_predict(distilbert_classifier, val_dataloader)\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:387: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import BertConfig\n",
    "config = BertConfig(\n",
    "    vocab_size=102025, \n",
    "    hidden_size=192,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=6,\n",
    "    intermediate_size=1024,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    layer_norm_eps=1e-12,\n",
    "    gradient_checkpointing=True,\n",
    "    initializer_range=0.02\n",
    ")\n",
    "\n",
    "student_model = BertForSequenceClassification(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 10\n",
    "\n",
    "def dist_loss(s, t, label):\n",
    "    s_logits = s.logits\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    # label_one_hot = F.one_hot(label, num_classes=num_classes).float()\n",
    "\n",
    "    # Compute the log probabilities and distillation loss for hard targets\n",
    "    # log_prob_s1 = F.log_softmax(s_logits, dim=1)\n",
    "    # dist_loss1 = -(label_one_hot * log_prob_s1).sum(dim=1).mean()\n",
    "    dist_loss1 = nn.CrossEntropyLoss(s, label)\n",
    "    # Compute the probabilities and log probabilities for soft targets\n",
    "    prob_t = F.softmax(t.logits / temperature, dim=1)\n",
    "    log_prob_s2 = F.log_softmax(s_logits / temperature, dim=1)\n",
    "\n",
    "    # Compute the distillation loss for soft targets\n",
    "    dist_loss2 = -(prob_t * log_prob_s2).sum(dim=1).mean()\n",
    "\n",
    "    # Combine the losses\n",
    "    loss = dist_loss1 + dist_loss2\n",
    "\n",
    "    return loss\n",
    "\n",
    "# def dist_loss(s, t, label):\n",
    "\n",
    "#     s = s.logits\n",
    "    \n",
    "#     label = F.one_hot(label, num_classes=6)\n",
    "#     log_prob_s1 = F.log_softmax(s, dim=1)\n",
    "#     dist_loss1 = -(label*log_prob_s1).sum(dim=2).mean()\n",
    "    \n",
    "#     prob_t = F.softmax(t/temperature, dim=1)\n",
    "#     log_prob_s2 = F.log_softmax(s/temperature, dim=1) \n",
    "#     # print(prob_t.size()) \n",
    "#     # print(log_prob_s2.size()) \n",
    "#     dist_loss2 = -(prob_t*log_prob_s2).sum(dim=2).mean()\n",
    "   \n",
    "#     loss = dist_loss1 + dist_loss2\n",
    "#     # print(type(loss))\n",
    "#     # print(loss.size())\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distilled_evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask).logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "def distillation_train(student_model, train_dataloader, val_dataloader=None, epochs=2, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        student_model.to(\"cuda\")\n",
    "        # Put the model into the training mode\n",
    "        student_model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = distilbert_classifier(b_input_ids, b_attn_mask)\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            \n",
    "            logits = student_model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            \n",
    "            # loss = loss_fn(logits, b_labels)\n",
    "            loss = dist_loss(logits, teacher_outputs, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = distilled_evaluate(student_model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdistillation_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[110], line 49\u001b[0m, in \u001b[0;36mdistillation_train\u001b[1;34m(student_model, train_dataloader, val_dataloader, epochs, evaluation)\u001b[0m\n\u001b[0;32m     44\u001b[0m logits \u001b[38;5;241m=\u001b[39m student_model(b_input_ids, b_attn_mask)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Compute loss and accumulate the loss values\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# loss = loss_fn(logits, b_labels)\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mdist_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     51\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[108], line 12\u001b[0m, in \u001b[0;36mdist_loss\u001b[1;34m(s, t, label)\u001b[0m\n\u001b[0;32m      4\u001b[0m s_logits \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert labels to one-hot encoding\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# label_one_hot = F.one_hot(label, num_classes=num_classes).float()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Compute the log probabilities and distillation loss for hard targets\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# log_prob_s1 = F.log_softmax(s_logits, dim=1)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# dist_loss1 = -(label_one_hot * log_prob_s1).sum(dim=1).mean()\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m dist_loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Compute the probabilities and log probabilities for soft targets\u001b[39;00m\n\u001b[0;32m     14\u001b[0m prob_t \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(t\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1169\u001b[0m, in \u001b[0;36mCrossEntropyLoss.__init__\u001b[1;34m(self, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m   1168\u001b[0m              reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, label_smoothing: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index \u001b[38;5;241m=\u001b[39m ignore_index\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing \u001b[38;5;241m=\u001b[39m label_smoothing\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:30\u001b[0m, in \u001b[0;36m_WeightedLoss.__init__\u001b[1;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, weight)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight: Optional[Tensor]\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:23\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[1;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_get_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\_reduction.py:35\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[1;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[0;32m     36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "distillation_train(student_model, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
